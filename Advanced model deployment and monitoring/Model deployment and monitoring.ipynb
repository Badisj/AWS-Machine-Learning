{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - pytorch\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n",
      "\n",
      "LibMambaUnsatisfiableError: Encountered problems while solving:\n",
      "  - package pytorch-1.6.0-py3.6_cpu_0 requires python >=3.6,<3.7.0a0, but none of the providers can be installed\n",
      "\n",
      "Could not solve for environment specs\n",
      "The following packages are incompatible\n",
      "├─ \u001b[32mpin-1\u001b[0m is installable and it requires\n",
      "│  └─ \u001b[32mpython 3.11.* \u001b[0m, which can be installed;\n",
      "└─ \u001b[31mpytorch 1.6.0 \u001b[0m is not installable because there are no viable options\n",
      "   ├─ \u001b[31mpytorch 1.6.0\u001b[0m would require\n",
      "   │  └─ \u001b[31mpython >=3.6,<3.7.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "   ├─ \u001b[31mpytorch 1.6.0\u001b[0m would require\n",
      "   │  └─ \u001b[31mpython >=3.7,<3.8.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "   ├─ \u001b[31mpytorch 1.6.0\u001b[0m would require\n",
      "   │  └─ \u001b[31mpython >=3.8,<3.9.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "   └─ \u001b[31mpytorch 1.6.0\u001b[0m would require\n",
      "      └─ \u001b[31mpython >=3.9,<3.10.0a0 \u001b[0m, which conflicts with any installable versions previously reported.\n",
      "\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[2 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m Failed to find sentencepiece pkgconfig\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!conda install -q -y pytorch==1.6.0 -c pytorch\n",
    "!pip install --disable-pip-version-check -q transformers==3.5.1\n",
    "!pip install -q protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== low-level service client of the boto3 session ==========================\n",
    "config = botocore.config.Config(user_agent_extra='bedissj-1699438736259')\n",
    "\n",
    "\n",
    "sm = boto3.client(service_name='sagemaker', \n",
    "                  config=config)\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime',\n",
    "                          config=config)\n",
    "\n",
    "sess = sagemaker.Session(sagemaker_client=sm,\n",
    "                         sagemaker_runtime_client=sm_runtime)\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "cw = boto3.client(service_name='cloudwatch', \n",
    "                  config=config)\n",
    "\n",
    "autoscale = boto3.client(service_name=\"application-autoscaling\", \n",
    "                         config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an endpoint with multiple variants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_s3_uri = \"s3://{}/y8a6c91lumhh-ModelTra-GTH766MIe3-002-1a2991b7/output/model.tar.gz\".format(bucket)\n",
    "model_b_s3_uri = \"s3://{}/y8a6c91lumhh-ModelTra-GTH766MIe3-010-b1656de1/output/model.tar.gz\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Construct Docker Image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK_VERSION = '0.20.0'\n",
    "deploy_instance_type = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659782779980.dkr.ecr.eu-west-3.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "churn_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='sklearn',\n",
    "    version=FRAMEWORK_VERSION,\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\",\n",
    "    region=region\n",
    ")\n",
    "\n",
    "print(churn_inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "model_name_a = \"churn-prediction-mon1-model-a-{}\".format(timestamp)\n",
    "model_name_b = \"churn-prediction-mon1-model-b-{}\".format(timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if models already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_existence(model_name):\n",
    "    for model in sm.list_models()['Models']:\n",
    "        if model_name == model['ModelName']:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.sklearn.model.SKLearnModel object at 0x7fd330c78a10>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel, FrameworkModel\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "if not check_model_existence(model_name=model_name_a):\n",
    "    model_a = SKLearnModel(\n",
    "        name=model_name_a,\n",
    "        model_data=model_a_s3_uri,\n",
    "        image_uri=churn_inference_image_uri,\n",
    "        entry_point='./src/inference.py',\n",
    "        sagemaker_session=sess,\n",
    "        role=role\n",
    "    )\n",
    "    pprint(model_a)\n",
    "\n",
    "else:\n",
    "    print(\"Model {} already exists\".format(model_name_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model.FrameworkModel object at 0x7fd330f0f610>\n"
     ]
    }
   ],
   "source": [
    "if not check_model_existence(model_name=model_name_b):\n",
    "    # model_b = sm.create_model(\n",
    "    #     ModelName=model_name_b,\n",
    "    #     ExecutionRoleArn=role,\n",
    "    #     PrimaryContainer={\n",
    "    #         \"ModelDataUrl\": model_b_s3_uri,\n",
    "    #         \"Image\":churn_inference_image_uri,\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    model_b = FrameworkModel(\n",
    "        name=model_name_b,\n",
    "        model_data=model_b_s3_uri,\n",
    "        image_uri=churn_inference_image_uri,\n",
    "        entry_point='./src/inference.py',\n",
    "        sagemaker_session=sess,\n",
    "        role=role\n",
    "    )\n",
    "    pprint(model_b)\n",
    "\n",
    "else:\n",
    "    print(\"Model {} already exists\".format(model_name_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create production variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelName': 'churn-prediction-mon1-model-a-1728467686', 'InstanceType': 'ml.m5.large', 'InitialInstanceCount': 1, 'VariantName': 'VariantA', 'InitialVariantWeight': 50}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "variantA = production_variant(\n",
    "    model_name=model_name_a,\n",
    "    instance_type=deploy_instance_type,\n",
    "    initial_instance_count=1,\n",
    "    initial_weight=50,\n",
    "    variant_name='VariantA'\n",
    ")\n",
    "print(variantA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelName': 'churn-prediction-mon1-model-a-1728467686', 'InstanceType': 'ml.m5.large', 'InitialInstanceCount': 1, 'VariantName': 'VariantA', 'InitialVariantWeight': 50}\n"
     ]
    }
   ],
   "source": [
    "variantB = production_variant(\n",
    "    model_name=model_name_b,\n",
    "    instance_type=deploy_instance_type,\n",
    "    initial_instance_count=1,\n",
    "    initial_weight=50,\n",
    "    variant_name='VariantB'\n",
    ")\n",
    "print(variantA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configure and create the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check endpoint configuration existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_endpoint_config_existence(endpoint_config_name):\n",
    "    for endpoint_config in sm.list_endpoint_configs()['EndpointConfigs']:\n",
    "        if endpoint_config_name == endpoint_config['EndpointConfigName']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_endpoint_existence(endpoint_name):\n",
    "    for endpoint in sm.list_endpoints()['Endpoints']:\n",
    "        if endpoint_name == endpoint['EndpointName']:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint configuration for  A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Could not find model \"churn-prediction-mon1-model-a-1728467686\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m endpoint_config_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchurn-prediction-ab-epc-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_endpoint_config_existence(endpoint_config_name):\n\u001b[0;32m----> 4\u001b[0m     endpoint_config \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointConfigName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mProductionVariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvariantA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariantB\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     pprint(endpoint_config)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Could not find model \"churn-prediction-mon1-model-a-1728467686\"."
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"churn-prediction-ab-epc-{}\".format(timestamp)\n",
    "\n",
    "if not check_endpoint_config_existence(endpoint_config_name):\n",
    "    endpoint_config = sm.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[variantA, variantB]\n",
    "    )\n",
    "    pprint(endpoint_config)\n",
    "else: \n",
    "     print(\"Endpoint configuration {} already exists\".format(endpoint_config_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Models': [],\n",
       " 'ResponseMetadata': {'RequestId': 'df7aaca5-0d29-4b35-9fe3-8f110aa2b029',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'df7aaca5-0d29-4b35-9fe3-8f110aa2b029',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '13',\n",
       "   'date': 'Wed, 09 Oct 2024 10:13:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = Model(\n",
    "        name=\"test\",\n",
    "        model_data=model_a_s3_uri,\n",
    "        image_uri=churn_inference_image_uri,\n",
    "        sagemaker_session=sess,\n",
    "        role=role\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Creates a model in SageMaker. In the request, you name the model and describe a primary container. For the primary container, you specify the Docker image that contains inference code, artifacts (from prior training), and a custom environment map that the inference code uses when you deploy the model for predictions.\n",
      "\n",
      " \n",
      "\n",
      "Use this API to create a model if you want to use SageMaker hosting services or run a batch transform job.\n",
      "\n",
      " \n",
      "\n",
      "To host your model, you create an endpoint configuration with the ``CreateEndpointConfig`` API, and then create an endpoint with the ``CreateEndpoint`` API. SageMaker then deploys all of the containers that you defined for the model in the hosting environment.\n",
      "\n",
      " \n",
      "\n",
      "To run a batch transform using your model, you start a job with the ``CreateTransformJob`` API. SageMaker uses your model and your dataset to get inferences which are then saved to a specified S3 location.\n",
      "\n",
      " \n",
      "\n",
      "In the request, you also provide an IAM role that SageMaker can assume to access model artifacts and docker image for deployment on ML compute hosting instances or for batch transform jobs. In addition, you also use the IAM role to manage permissions the inference code needs. For example, if the inference code access any other Amazon Web Services resources, you grant necessary permissions via this role.\n",
      "\n",
      "\n",
      "\n",
      "See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/sagemaker-2017-07-24/CreateModel>`_\n",
      "\n",
      "\n",
      "**Request Syntax**\n",
      "::\n",
      "\n",
      "  response = client.create_model(\n",
      "      ModelName='string',\n",
      "      PrimaryContainer={\n",
      "          'ContainerHostname': 'string',\n",
      "          'Image': 'string',\n",
      "          'ImageConfig': {\n",
      "              'RepositoryAccessMode': 'Platform'|'Vpc',\n",
      "              'RepositoryAuthConfig': {\n",
      "                  'RepositoryCredentialsProviderArn': 'string'\n",
      "              }\n",
      "          },\n",
      "          'Mode': 'SingleModel'|'MultiModel',\n",
      "          'ModelDataUrl': 'string',\n",
      "          'ModelDataSource': {\n",
      "              'S3DataSource': {\n",
      "                  'S3Uri': 'string',\n",
      "                  'S3DataType': 'S3Prefix'|'S3Object',\n",
      "                  'CompressionType': 'None'|'Gzip',\n",
      "                  'ModelAccessConfig': {\n",
      "                      'AcceptEula': True|False\n",
      "                  },\n",
      "                  'HubAccessConfig': {\n",
      "                      'HubContentArn': 'string'\n",
      "                  }\n",
      "              }\n",
      "          },\n",
      "          'Environment': {\n",
      "              'string': 'string'\n",
      "          },\n",
      "          'ModelPackageName': 'string',\n",
      "          'InferenceSpecificationName': 'string',\n",
      "          'MultiModelConfig': {\n",
      "              'ModelCacheSetting': 'Enabled'|'Disabled'\n",
      "          }\n",
      "      },\n",
      "      Containers=[\n",
      "          {\n",
      "              'ContainerHostname': 'string',\n",
      "              'Image': 'string',\n",
      "              'ImageConfig': {\n",
      "                  'RepositoryAccessMode': 'Platform'|'Vpc',\n",
      "                  'RepositoryAuthConfig': {\n",
      "                      'RepositoryCredentialsProviderArn': 'string'\n",
      "                  }\n",
      "              },\n",
      "              'Mode': 'SingleModel'|'MultiModel',\n",
      "              'ModelDataUrl': 'string',\n",
      "              'ModelDataSource': {\n",
      "                  'S3DataSource': {\n",
      "                      'S3Uri': 'string',\n",
      "                      'S3DataType': 'S3Prefix'|'S3Object',\n",
      "                      'CompressionType': 'None'|'Gzip',\n",
      "                      'ModelAccessConfig': {\n",
      "                          'AcceptEula': True|False\n",
      "                      },\n",
      "                      'HubAccessConfig': {\n",
      "                          'HubContentArn': 'string'\n",
      "                      }\n",
      "                  }\n",
      "              },\n",
      "              'Environment': {\n",
      "                  'string': 'string'\n",
      "              },\n",
      "              'ModelPackageName': 'string',\n",
      "              'InferenceSpecificationName': 'string',\n",
      "              'MultiModelConfig': {\n",
      "                  'ModelCacheSetting': 'Enabled'|'Disabled'\n",
      "              }\n",
      "          },\n",
      "      ],\n",
      "      InferenceExecutionConfig={\n",
      "          'Mode': 'Serial'|'Direct'\n",
      "      },\n",
      "      ExecutionRoleArn='string',\n",
      "      Tags=[\n",
      "          {\n",
      "              'Key': 'string',\n",
      "              'Value': 'string'\n",
      "          },\n",
      "      ],\n",
      "      VpcConfig={\n",
      "          'SecurityGroupIds': [\n",
      "              'string',\n",
      "          ],\n",
      "          'Subnets': [\n",
      "              'string',\n",
      "          ]\n",
      "      },\n",
      "      EnableNetworkIsolation=True|False\n",
      "  )\n",
      "  \n",
      ":type ModelName: string\n",
      ":param ModelName: **[REQUIRED]** \n",
      "\n",
      "  The name of the new model.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      ":type PrimaryContainer: dict\n",
      ":param PrimaryContainer: \n",
      "\n",
      "  The location of the primary docker image containing inference code, associated artifacts, and custom environment map that the inference code uses when the model is deployed for predictions.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  - **ContainerHostname** *(string) --* \n",
      "\n",
      "    This parameter is ignored for models that contain only a ``PrimaryContainer``.\n",
      "\n",
      "     \n",
      "\n",
      "    When a ``ContainerDefinition`` is part of an inference pipeline, the value of the parameter uniquely identifies the container for the purposes of logging and metrics. For information, see `Use Logs and Metrics to Monitor an Inference Pipeline <https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-logs-metrics.html>`__. If you don't specify a value for this parameter for a ``ContainerDefinition`` that is part of an inference pipeline, a unique name is automatically assigned based on the position of the ``ContainerDefinition`` in the pipeline. If you specify a value for the ``ContainerHostName`` for any ``ContainerDefinition`` that is part of an inference pipeline, you must specify a value for the ``ContainerHostName`` parameter of every ``ContainerDefinition`` in that pipeline.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **Image** *(string) --* \n",
      "\n",
      "    The path where inference code is stored. This can be either in Amazon EC2 Container Registry or in a Docker registry that is accessible from the same VPC that you configure for your endpoint. If you are using your own custom algorithm instead of an algorithm provided by SageMaker, the inference code must meet SageMaker requirements. SageMaker supports both ``registry/repository[:tag]`` and ``registry/repository[@digest]`` image path formats. For more information, see `Using Your Own Algorithms with Amazon SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.\n",
      "\n",
      "     \n",
      "\n",
      "    .. note::\n",
      "\n",
      "      \n",
      "\n",
      "      The model artifacts in an Amazon S3 bucket and the Docker image for inference container in Amazon EC2 Container Registry must be in the same region as the model or endpoint you are creating.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **ImageConfig** *(dict) --* \n",
      "\n",
      "    Specifies whether the model container is in Amazon ECR or a private Docker registry accessible from your Amazon Virtual Private Cloud (VPC). For information about storing containers in a private Docker registry, see `Use a Private Docker Registry for Real-Time Inference Containers <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html>`__.\n",
      "\n",
      "     \n",
      "\n",
      "    .. note::\n",
      "\n",
      "      \n",
      "\n",
      "      The model artifacts in an Amazon S3 bucket and the Docker image for inference container in Amazon EC2 Container Registry must be in the same region as the model or endpoint you are creating.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - **RepositoryAccessMode** *(string) --* **[REQUIRED]** \n",
      "\n",
      "      Set this to one of the following values:\n",
      "\n",
      "       \n",
      "\n",
      "      \n",
      "      * ``Platform`` - The model image is hosted in Amazon ECR.\n",
      "       \n",
      "      * ``Vpc`` - The model image is hosted in a private Docker registry in your VPC.\n",
      "      \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **RepositoryAuthConfig** *(dict) --* \n",
      "\n",
      "      (Optional) Specifies an authentication configuration for the private docker registry where your model image is hosted. Specify a value for this property only if you specified ``Vpc`` as the value for the ``RepositoryAccessMode`` field, and the private Docker registry where the model image is hosted requires authentication.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - **RepositoryCredentialsProviderArn** *(string) --* **[REQUIRED]** \n",
      "\n",
      "        The Amazon Resource Name (ARN) of an Amazon Web Services Lambda function that provides credentials to authenticate to the private Docker registry where your model image is hosted. For information about how to create an Amazon Web Services Lambda function, see `Create a Lambda function with the console <https://docs.aws.amazon.com/lambda/latest/dg/getting-started-create-function.html>`__ in the *Amazon Web Services Lambda Developer Guide*.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "    \n",
      "  \n",
      "  - **Mode** *(string) --* \n",
      "\n",
      "    Whether the container hosts a single model or multiple models.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **ModelDataUrl** *(string) --* \n",
      "\n",
      "    The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single gzip compressed tar archive (.tar.gz suffix). The S3 path is required for SageMaker built-in algorithms, but not if you use your own algorithms. For more information on built-in algorithms, see `Common Parameters <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.\n",
      "\n",
      "     \n",
      "\n",
      "    .. note::\n",
      "\n",
      "      \n",
      "\n",
      "      The model artifacts must be in an S3 bucket that is in the same region as the model or endpoint you are creating.\n",
      "\n",
      "      \n",
      "\n",
      "     \n",
      "\n",
      "    If you provide a value for this parameter, SageMaker uses Amazon Web Services Security Token Service to download model artifacts from the S3 path you provide. Amazon Web Services STS is activated in your Amazon Web Services account by default. If you previously deactivated Amazon Web Services STS for a region, you need to reactivate Amazon Web Services STS for that region. For more information, see `Activating and Deactivating Amazon Web Services STS in an Amazon Web Services Region <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html>`__ in the *Amazon Web Services Identity and Access Management User Guide*.\n",
      "\n",
      "     \n",
      "\n",
      "    .. warning::\n",
      "\n",
      "       \n",
      "\n",
      "      If you use a built-in algorithm to create a model, SageMaker requires that you provide a S3 path to the model artifacts in ``ModelDataUrl``.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **ModelDataSource** *(dict) --* \n",
      "\n",
      "    Specifies the location of ML model data to deploy.\n",
      "\n",
      "     \n",
      "\n",
      "    .. note::\n",
      "\n",
      "      \n",
      "\n",
      "      Currently you cannot use ``ModelDataSource`` in conjunction with SageMaker batch transform, SageMaker serverless endpoints, SageMaker multi-model endpoints, and SageMaker Marketplace.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - **S3DataSource** *(dict) --* \n",
      "\n",
      "      Specifies the S3 location of ML model data to deploy.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - **S3Uri** *(string) --* **[REQUIRED]** \n",
      "\n",
      "        Specifies the S3 path of ML model data to deploy.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "      - **S3DataType** *(string) --* **[REQUIRED]** \n",
      "\n",
      "        Specifies the type of ML model data to deploy.\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose ``S3Prefix``, ``S3Uri`` identifies a key name prefix. SageMaker uses all objects that match the specified key name prefix as part of the ML model data to deploy. A valid key name prefix identified by ``S3Uri`` always ends with a forward slash (/).\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose ``S3Object``, ``S3Uri`` identifies an object that is the ML model data to deploy.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "      - **CompressionType** *(string) --* **[REQUIRED]** \n",
      "\n",
      "        Specifies how the ML model data is prepared.\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose ``Gzip`` and choose ``S3Object`` as the value of ``S3DataType``, ``S3Uri`` identifies an object that is a gzip-compressed TAR archive. SageMaker will attempt to decompress and untar the object during model deployment.\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose ``None`` and chooose ``S3Object`` as the value of ``S3DataType``, ``S3Uri`` identifies an object that represents an uncompressed ML model to deploy.\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose None and choose ``S3Prefix`` as the value of ``S3DataType``, ``S3Uri`` identifies a key name prefix, under which all objects represents the uncompressed ML model to deploy.\n",
      "\n",
      "         \n",
      "\n",
      "        If you choose None, then SageMaker will follow rules below when creating model data files under /opt/ml/model directory for use by your inference code:\n",
      "\n",
      "         \n",
      "\n",
      "        \n",
      "        * If you choose ``S3Object`` as the value of ``S3DataType``, then SageMaker will split the key of the S3 object referenced by ``S3Uri`` by slash (/), and use the last part as the filename of the file holding the content of the S3 object.\n",
      "         \n",
      "        * If you choose ``S3Prefix`` as the value of ``S3DataType``, then for each S3 object under the key name pefix referenced by ``S3Uri``, SageMaker will trim its key by the prefix, and use the remainder as the path (relative to ``/opt/ml/model``) of the file holding the content of the S3 object. SageMaker will split the remainder by slash (/), using intermediate parts as directory names and the last part as filename of the file holding the content of the S3 object.\n",
      "         \n",
      "        * Do not use any of the following as file names or directory names: \n",
      "\n",
      "          \n",
      "          * An empty or blank string\n",
      "           \n",
      "          * A string which contains null bytes\n",
      "           \n",
      "          * A string longer than 255 bytes\n",
      "           \n",
      "          * A single dot ( ``.``)\n",
      "           \n",
      "          * A double dot ( ``..``)\n",
      "          \n",
      "\n",
      "        \n",
      "         \n",
      "        * Ambiguous file names will result in model deployment failure. For example, if your uncompressed ML model consists of two S3 objects ``s3://mybucket/model/weights`` and ``s3://mybucket/model/weights/part1`` and you specify ``s3://mybucket/model/`` as the value of ``S3Uri`` and ``S3Prefix`` as the value of ``S3DataType``, then it will result in name clash between ``/opt/ml/model/weights`` (a regular file) and ``/opt/ml/model/weights/`` (a directory).\n",
      "         \n",
      "        * Do not organize the model artifacts in `S3 console using folders <https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html>`__. When you create a folder in S3 console, S3 creates a 0-byte object with a key set to the folder name you provide. They key of the 0-byte object ends with a slash (/) which violates SageMaker restrictions on model artifact file names, leading to model deployment failure.\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "      - **ModelAccessConfig** *(dict) --* \n",
      "\n",
      "        Specifies the access configuration file for the ML model. You can explicitly accept the model end-user license agreement (EULA) within the ``ModelAccessConfig``. You are responsible for reviewing and complying with any applicable license terms and making sure they are acceptable for your use case before downloading or using a model.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "        - **AcceptEula** *(boolean) --* **[REQUIRED]** \n",
      "\n",
      "          Specifies agreement to the model end-user license agreement (EULA). The ``AcceptEula`` value must be explicitly defined as ``True`` in order to accept the EULA that this model requires. You are responsible for reviewing and complying with any applicable license terms and making sure they are acceptable for your use case before downloading or using a model.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "      \n",
      "      - **HubAccessConfig** *(dict) --* \n",
      "\n",
      "        Configuration information for hub access.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "        - **HubContentArn** *(string) --* **[REQUIRED]** \n",
      "\n",
      "          The ARN of the hub content for which deployment access is allowed.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "      \n",
      "    \n",
      "  \n",
      "  - **Environment** *(dict) --* \n",
      "\n",
      "    The environment variables to set in the Docker container.\n",
      "\n",
      "     \n",
      "\n",
      "    The maximum length of each key and value in the ``Environment`` map is 1024 bytes. The maximum length of all keys and values in the map, combined, is 32 KB. If you pass multiple containers to a ``CreateModel`` request, then the maximum length of all of their maps, combined, is also 32 KB.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - *(string) --* \n",
      "\n",
      "    \n",
      "      - *(string) --* \n",
      "\n",
      "      \n",
      "\n",
      "\n",
      "  - **ModelPackageName** *(string) --* \n",
      "\n",
      "    The name or Amazon Resource Name (ARN) of the model package to use to create the model.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **InferenceSpecificationName** *(string) --* \n",
      "\n",
      "    The inference specification name in the model package version.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "  - **MultiModelConfig** *(dict) --* \n",
      "\n",
      "    Specifies additional configuration for multi-model endpoints.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - **ModelCacheSetting** *(string) --* \n",
      "\n",
      "      Whether to cache models for a multi-model endpoint. By default, multi-model endpoints cache models so that a model does not have to be loaded into memory each time it is invoked. Some use cases do not benefit from model caching. For example, if an endpoint hosts a large number of models that are each invoked infrequently, the endpoint might perform better if you disable model caching. To disable model caching, set the value of this parameter to ``Disabled``.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      ":type Containers: list\n",
      ":param Containers: \n",
      "\n",
      "  Specifies the containers in the inference pipeline.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  - *(dict) --* \n",
      "\n",
      "    Describes the container, as part of model definition.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - **ContainerHostname** *(string) --* \n",
      "\n",
      "      This parameter is ignored for models that contain only a ``PrimaryContainer``.\n",
      "\n",
      "       \n",
      "\n",
      "      When a ``ContainerDefinition`` is part of an inference pipeline, the value of the parameter uniquely identifies the container for the purposes of logging and metrics. For information, see `Use Logs and Metrics to Monitor an Inference Pipeline <https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-logs-metrics.html>`__. If you don't specify a value for this parameter for a ``ContainerDefinition`` that is part of an inference pipeline, a unique name is automatically assigned based on the position of the ``ContainerDefinition`` in the pipeline. If you specify a value for the ``ContainerHostName`` for any ``ContainerDefinition`` that is part of an inference pipeline, you must specify a value for the ``ContainerHostName`` parameter of every ``ContainerDefinition`` in that pipeline.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **Image** *(string) --* \n",
      "\n",
      "      The path where inference code is stored. This can be either in Amazon EC2 Container Registry or in a Docker registry that is accessible from the same VPC that you configure for your endpoint. If you are using your own custom algorithm instead of an algorithm provided by SageMaker, the inference code must meet SageMaker requirements. SageMaker supports both ``registry/repository[:tag]`` and ``registry/repository[@digest]`` image path formats. For more information, see `Using Your Own Algorithms with Amazon SageMaker <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html>`__.\n",
      "\n",
      "       \n",
      "\n",
      "      .. note::\n",
      "\n",
      "        \n",
      "\n",
      "        The model artifacts in an Amazon S3 bucket and the Docker image for inference container in Amazon EC2 Container Registry must be in the same region as the model or endpoint you are creating.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **ImageConfig** *(dict) --* \n",
      "\n",
      "      Specifies whether the model container is in Amazon ECR or a private Docker registry accessible from your Amazon Virtual Private Cloud (VPC). For information about storing containers in a private Docker registry, see `Use a Private Docker Registry for Real-Time Inference Containers <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html>`__.\n",
      "\n",
      "       \n",
      "\n",
      "      .. note::\n",
      "\n",
      "        \n",
      "\n",
      "        The model artifacts in an Amazon S3 bucket and the Docker image for inference container in Amazon EC2 Container Registry must be in the same region as the model or endpoint you are creating.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - **RepositoryAccessMode** *(string) --* **[REQUIRED]** \n",
      "\n",
      "        Set this to one of the following values:\n",
      "\n",
      "         \n",
      "\n",
      "        \n",
      "        * ``Platform`` - The model image is hosted in Amazon ECR.\n",
      "         \n",
      "        * ``Vpc`` - The model image is hosted in a private Docker registry in your VPC.\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "      - **RepositoryAuthConfig** *(dict) --* \n",
      "\n",
      "        (Optional) Specifies an authentication configuration for the private docker registry where your model image is hosted. Specify a value for this property only if you specified ``Vpc`` as the value for the ``RepositoryAccessMode`` field, and the private Docker registry where the model image is hosted requires authentication.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "        - **RepositoryCredentialsProviderArn** *(string) --* **[REQUIRED]** \n",
      "\n",
      "          The Amazon Resource Name (ARN) of an Amazon Web Services Lambda function that provides credentials to authenticate to the private Docker registry where your model image is hosted. For information about how to create an Amazon Web Services Lambda function, see `Create a Lambda function with the console <https://docs.aws.amazon.com/lambda/latest/dg/getting-started-create-function.html>`__ in the *Amazon Web Services Lambda Developer Guide*.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "      \n",
      "    \n",
      "    - **Mode** *(string) --* \n",
      "\n",
      "      Whether the container hosts a single model or multiple models.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **ModelDataUrl** *(string) --* \n",
      "\n",
      "      The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single gzip compressed tar archive (.tar.gz suffix). The S3 path is required for SageMaker built-in algorithms, but not if you use your own algorithms. For more information on built-in algorithms, see `Common Parameters <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html>`__.\n",
      "\n",
      "       \n",
      "\n",
      "      .. note::\n",
      "\n",
      "        \n",
      "\n",
      "        The model artifacts must be in an S3 bucket that is in the same region as the model or endpoint you are creating.\n",
      "\n",
      "        \n",
      "\n",
      "       \n",
      "\n",
      "      If you provide a value for this parameter, SageMaker uses Amazon Web Services Security Token Service to download model artifacts from the S3 path you provide. Amazon Web Services STS is activated in your Amazon Web Services account by default. If you previously deactivated Amazon Web Services STS for a region, you need to reactivate Amazon Web Services STS for that region. For more information, see `Activating and Deactivating Amazon Web Services STS in an Amazon Web Services Region <https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html>`__ in the *Amazon Web Services Identity and Access Management User Guide*.\n",
      "\n",
      "       \n",
      "\n",
      "      .. warning::\n",
      "\n",
      "         \n",
      "\n",
      "        If you use a built-in algorithm to create a model, SageMaker requires that you provide a S3 path to the model artifacts in ``ModelDataUrl``.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **ModelDataSource** *(dict) --* \n",
      "\n",
      "      Specifies the location of ML model data to deploy.\n",
      "\n",
      "       \n",
      "\n",
      "      .. note::\n",
      "\n",
      "        \n",
      "\n",
      "        Currently you cannot use ``ModelDataSource`` in conjunction with SageMaker batch transform, SageMaker serverless endpoints, SageMaker multi-model endpoints, and SageMaker Marketplace.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - **S3DataSource** *(dict) --* \n",
      "\n",
      "        Specifies the S3 location of ML model data to deploy.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "        - **S3Uri** *(string) --* **[REQUIRED]** \n",
      "\n",
      "          Specifies the S3 path of ML model data to deploy.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "        - **S3DataType** *(string) --* **[REQUIRED]** \n",
      "\n",
      "          Specifies the type of ML model data to deploy.\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose ``S3Prefix``, ``S3Uri`` identifies a key name prefix. SageMaker uses all objects that match the specified key name prefix as part of the ML model data to deploy. A valid key name prefix identified by ``S3Uri`` always ends with a forward slash (/).\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose ``S3Object``, ``S3Uri`` identifies an object that is the ML model data to deploy.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "        - **CompressionType** *(string) --* **[REQUIRED]** \n",
      "\n",
      "          Specifies how the ML model data is prepared.\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose ``Gzip`` and choose ``S3Object`` as the value of ``S3DataType``, ``S3Uri`` identifies an object that is a gzip-compressed TAR archive. SageMaker will attempt to decompress and untar the object during model deployment.\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose ``None`` and chooose ``S3Object`` as the value of ``S3DataType``, ``S3Uri`` identifies an object that represents an uncompressed ML model to deploy.\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose None and choose ``S3Prefix`` as the value of ``S3DataType``, ``S3Uri`` identifies a key name prefix, under which all objects represents the uncompressed ML model to deploy.\n",
      "\n",
      "           \n",
      "\n",
      "          If you choose None, then SageMaker will follow rules below when creating model data files under /opt/ml/model directory for use by your inference code:\n",
      "\n",
      "           \n",
      "\n",
      "          \n",
      "          * If you choose ``S3Object`` as the value of ``S3DataType``, then SageMaker will split the key of the S3 object referenced by ``S3Uri`` by slash (/), and use the last part as the filename of the file holding the content of the S3 object.\n",
      "           \n",
      "          * If you choose ``S3Prefix`` as the value of ``S3DataType``, then for each S3 object under the key name pefix referenced by ``S3Uri``, SageMaker will trim its key by the prefix, and use the remainder as the path (relative to ``/opt/ml/model``) of the file holding the content of the S3 object. SageMaker will split the remainder by slash (/), using intermediate parts as directory names and the last part as filename of the file holding the content of the S3 object.\n",
      "           \n",
      "          * Do not use any of the following as file names or directory names: \n",
      "\n",
      "            \n",
      "            * An empty or blank string\n",
      "             \n",
      "            * A string which contains null bytes\n",
      "             \n",
      "            * A string longer than 255 bytes\n",
      "             \n",
      "            * A single dot ( ``.``)\n",
      "             \n",
      "            * A double dot ( ``..``)\n",
      "            \n",
      "\n",
      "          \n",
      "           \n",
      "          * Ambiguous file names will result in model deployment failure. For example, if your uncompressed ML model consists of two S3 objects ``s3://mybucket/model/weights`` and ``s3://mybucket/model/weights/part1`` and you specify ``s3://mybucket/model/`` as the value of ``S3Uri`` and ``S3Prefix`` as the value of ``S3DataType``, then it will result in name clash between ``/opt/ml/model/weights`` (a regular file) and ``/opt/ml/model/weights/`` (a directory).\n",
      "           \n",
      "          * Do not organize the model artifacts in `S3 console using folders <https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html>`__. When you create a folder in S3 console, S3 creates a 0-byte object with a key set to the folder name you provide. They key of the 0-byte object ends with a slash (/) which violates SageMaker restrictions on model artifact file names, leading to model deployment failure.\n",
      "          \n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "        - **ModelAccessConfig** *(dict) --* \n",
      "\n",
      "          Specifies the access configuration file for the ML model. You can explicitly accept the model end-user license agreement (EULA) within the ``ModelAccessConfig``. You are responsible for reviewing and complying with any applicable license terms and making sure they are acceptable for your use case before downloading or using a model.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "          - **AcceptEula** *(boolean) --* **[REQUIRED]** \n",
      "\n",
      "            Specifies agreement to the model end-user license agreement (EULA). The ``AcceptEula`` value must be explicitly defined as ``True`` in order to accept the EULA that this model requires. You are responsible for reviewing and complying with any applicable license terms and making sure they are acceptable for your use case before downloading or using a model.\n",
      "\n",
      "            \n",
      "\n",
      "          \n",
      "        \n",
      "        - **HubAccessConfig** *(dict) --* \n",
      "\n",
      "          Configuration information for hub access.\n",
      "\n",
      "          \n",
      "\n",
      "        \n",
      "          - **HubContentArn** *(string) --* **[REQUIRED]** \n",
      "\n",
      "            The ARN of the hub content for which deployment access is allowed.\n",
      "\n",
      "            \n",
      "\n",
      "          \n",
      "        \n",
      "      \n",
      "    \n",
      "    - **Environment** *(dict) --* \n",
      "\n",
      "      The environment variables to set in the Docker container.\n",
      "\n",
      "       \n",
      "\n",
      "      The maximum length of each key and value in the ``Environment`` map is 1024 bytes. The maximum length of all keys and values in the map, combined, is 32 KB. If you pass multiple containers to a ``CreateModel`` request, then the maximum length of all of their maps, combined, is also 32 KB.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - *(string) --* \n",
      "\n",
      "      \n",
      "        - *(string) --* \n",
      "\n",
      "        \n",
      "  \n",
      "\n",
      "    - **ModelPackageName** *(string) --* \n",
      "\n",
      "      The name or Amazon Resource Name (ARN) of the model package to use to create the model.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **InferenceSpecificationName** *(string) --* \n",
      "\n",
      "      The inference specification name in the model package version.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **MultiModelConfig** *(dict) --* \n",
      "\n",
      "      Specifies additional configuration for multi-model endpoints.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "      - **ModelCacheSetting** *(string) --* \n",
      "\n",
      "        Whether to cache models for a multi-model endpoint. By default, multi-model endpoints cache models so that a model does not have to be loaded into memory each time it is invoked. Some use cases do not benefit from model caching. For example, if an endpoint hosts a large number of models that are each invoked infrequently, the endpoint might perform better if you disable model caching. To disable model caching, set the value of this parameter to ``Disabled``.\n",
      "\n",
      "        \n",
      "\n",
      "      \n",
      "    \n",
      "  \n",
      "\n",
      ":type InferenceExecutionConfig: dict\n",
      ":param InferenceExecutionConfig: \n",
      "\n",
      "  Specifies details of how containers in a multi-container endpoint are called.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  - **Mode** *(string) --* **[REQUIRED]** \n",
      "\n",
      "    How containers in a multi-container are run. The following values are valid.\n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "    * ``SERIAL`` - Containers run as a serial pipeline.\n",
      "     \n",
      "    * ``DIRECT`` - Only the individual container that you specify is run.\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      ":type ExecutionRoleArn: string\n",
      ":param ExecutionRoleArn: \n",
      "\n",
      "  The Amazon Resource Name (ARN) of the IAM role that SageMaker can assume to access model artifacts and docker image for deployment on ML compute instances or for batch transform jobs. Deploying on ML compute instances is part of model hosting. For more information, see `SageMaker Roles <https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html>`__.\n",
      "\n",
      "   \n",
      "\n",
      "  .. note::\n",
      "\n",
      "    \n",
      "\n",
      "    To be able to pass this role to SageMaker, the caller of this API must have the ``iam:PassRole`` permission.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      ":type Tags: list\n",
      ":param Tags: \n",
      "\n",
      "  An array of key-value pairs. You can use tags to categorize your Amazon Web Services resources in different ways, for example, by purpose, owner, or environment. For more information, see `Tagging Amazon Web Services Resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`__.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  - *(dict) --* \n",
      "\n",
      "    A tag object that consists of a key and an optional value, used to manage metadata for SageMaker Amazon Web Services resources.\n",
      "\n",
      "     \n",
      "\n",
      "    You can add tags to notebook instances, training jobs, hyperparameter tuning jobs, batch transform jobs, models, labeling jobs, work teams, endpoint configurations, and endpoints. For more information on adding tags to SageMaker resources, see `AddTags <https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AddTags.html>`__.\n",
      "\n",
      "     \n",
      "\n",
      "    For more information on adding metadata to your Amazon Web Services resources with tagging, see `Tagging Amazon Web Services resources <https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>`__. For advice on best practices for managing Amazon Web Services resources with tagging, see `Tagging Best Practices\\: Implement an Effective Amazon Web Services Resource Tagging Strategy <https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf>`__.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - **Key** *(string) --* **[REQUIRED]** \n",
      "\n",
      "      The tag key. Tag keys must be unique per resource.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "    - **Value** *(string) --* **[REQUIRED]** \n",
      "\n",
      "      The tag value.\n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      ":type VpcConfig: dict\n",
      ":param VpcConfig: \n",
      "\n",
      "  A `VpcConfig <https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html>`__ object that specifies the VPC that you want your model to connect to. Control access to and from your model container by configuring the VPC. ``VpcConfig`` is used in hosting services and in batch transform. For more information, see `Protect Endpoints by Using an Amazon Virtual Private Cloud <https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html>`__ and `Protect Data in Batch Transform Jobs by Using an Amazon Virtual Private Cloud <https://docs.aws.amazon.com/sagemaker/latest/dg/batch-vpc.html>`__.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  - **SecurityGroupIds** *(list) --* **[REQUIRED]** \n",
      "\n",
      "    The VPC security group IDs, in the form ``sg-xxxxxxxx``. Specify the security groups for the VPC that is specified in the ``Subnets`` field.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - *(string) --* \n",
      "\n",
      "    \n",
      "\n",
      "  - **Subnets** *(list) --* **[REQUIRED]** \n",
      "\n",
      "    The ID of the subnets in the VPC to which you want to connect your training job or model. For information about the availability of specific instance types, see `Supported Instance Types and Availability Zones <https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html>`__.\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "    - *(string) --* \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      ":type EnableNetworkIsolation: boolean\n",
      ":param EnableNetworkIsolation: \n",
      "\n",
      "  Isolates the model container. No inbound or outbound network calls can be made to or from the model container.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      ":rtype: dict\n",
      ":returns: \n",
      "  \n",
      "  **Response Syntax**\n",
      "\n",
      "  \n",
      "  ::\n",
      "\n",
      "    {\n",
      "        'ModelArn': 'string'\n",
      "    }\n",
      "    \n",
      "  **Response Structure**\n",
      "\n",
      "  \n",
      "\n",
      "  - *(dict) --* \n",
      "    \n",
      "\n",
      "    - **ModelArn** *(string) --* \n",
      "\n",
      "      The ARN of the model created in SageMaker.\n",
      "\n",
      "      \n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.11/site-packages/botocore/client.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "sm.create_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
